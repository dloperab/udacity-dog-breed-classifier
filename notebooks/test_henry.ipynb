{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {use_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "### TODO: Write data loaders for training, validation, and test sets\n",
    "## Specify appropriate transforms, and batch_sizes\n",
    "\n",
    "TARGET_SIZE = (224, 224)\n",
    "BATCH_SIZE = 20\n",
    "DATA_DIR = \"dogImages\"\n",
    "\n",
    "standard_normalization = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "#standard_normalization = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                        #CustomRotation(angles=[30, 45, 20]),\n",
    "                                        transforms.RandomRotation(40),\n",
    "                                        transforms.Resize(TARGET_SIZE),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        # ColorTransform(\"hsv\"),\n",
    "                                        standard_normalization])\n",
    "                                        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "val_transforms = transforms.Compose([transforms.Resize(TARGET_SIZE),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        standard_normalization\n",
    "                                        #ColorTransform(\"hsv\"),\n",
    "                                        ])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(TARGET_SIZE),\n",
    "                                    transforms.ToTensor(),\n",
    "                                  standard_normalization\n",
    "                                    #ColorTransform(\"hsv\"),\n",
    "                                    ])\n",
    "\n",
    "\n",
    "train_data = datasets.ImageFolder(DATA_DIR + '/train', transform=train_transforms)\n",
    "val_data = datasets.ImageFolder(DATA_DIR + '/valid', transform=val_transforms)\n",
    "test_data = datasets.ImageFolder(DATA_DIR + '/test', transform=test_transforms)\n",
    "classes = train_data.classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "loaders_scratch = {}\n",
    "loaders_scratch[\"train\"] = train_loader\n",
    "loaders_scratch[\"valid\"] = val_loader\n",
    "loaders_scratch[\"test\"] = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()        \n",
    "        # max pool = 2, # input_size / max pool \n",
    "        self.conv_block_1 = self.conv_block(3, 16) \n",
    "        self.conv_block_2 = self.conv_block(16, 32)\n",
    "        self.conv_block_3 = self.conv_block(32, 64)\n",
    "        self.conv_block_4 = self.conv_block(64, 128)\n",
    "        self.conv_block_5 = self.conv_block(128, 256)\n",
    "        self.conv_block_6 = self.conv_block(256, 512)\n",
    "        # dropout layers\n",
    "        self.dropout1 = nn.Dropout(0.2)        \n",
    "        self.dropout2 = nn.Dropout(0.5)                \n",
    "        self.flatten = nn.Flatten()        \n",
    "        # fully connected layers\n",
    "        #self.fc1 = nn.Linear(256 * 7 * 7, 2000)\n",
    "        self.fc1 = nn.Linear(512 * 3 * 3, 2000)\n",
    "        self.fc2 = nn.Linear(2000,133)# 133 => num classes\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(),            \n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        ## extra features        \n",
    "        x = self.conv_block_1(x)        \n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.conv_block_3(x)\n",
    "        x = self.conv_block_4(x)\n",
    "        x = self.conv_block_5(x)\n",
    "        x = self.conv_block_6(x)                \n",
    "        # flatten features\n",
    "        x = self.dropout1(x)\n",
    "        x = self.flatten(x)\n",
    "        #x = x.view(-1, 512 * 1 * 1)                \n",
    "        x = F.relu(self.fc1(x))        \n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model_scratch = Net()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_scratch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "### TODO: select loss function\n",
    "criterion_scratch = nn.CrossEntropyLoss()\n",
    "\n",
    "### TODO: select optimizer\n",
    "optimizer_scratch = optim.SGD(model_scratch.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 4.660955 \tValidation Loss: 4.351301\n",
      "Validation loss decreased (inf --> 4.351301).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 4.271890 \tValidation Loss: 4.181582\n",
      "Validation loss decreased (4.351301 --> 4.181582).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 4.007847 \tValidation Loss: 3.923587\n",
      "Validation loss decreased (4.181582 --> 3.923587).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 3.802997 \tValidation Loss: 3.790495\n",
      "Validation loss decreased (3.923587 --> 3.790495).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 3.601889 \tValidation Loss: 3.583936\n",
      "Validation loss decreased (3.790495 --> 3.583936).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 3.429857 \tValidation Loss: 3.522250\n",
      "Validation loss decreased (3.583936 --> 3.522250).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 3.264986 \tValidation Loss: 3.581443\n",
      "Epoch: 8 \tTraining Loss: 3.112244 \tValidation Loss: 3.288750\n",
      "Validation loss decreased (3.522250 --> 3.288750).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 2.967093 \tValidation Loss: 3.090924\n",
      "Validation loss decreased (3.288750 --> 3.090924).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 2.836238 \tValidation Loss: 3.162925\n",
      "Epoch: 11 \tTraining Loss: 2.685213 \tValidation Loss: 3.282059\n",
      "Epoch: 12 \tTraining Loss: 2.570565 \tValidation Loss: 3.098731\n",
      "Epoch: 13 \tTraining Loss: 2.435489 \tValidation Loss: 3.055941\n",
      "Validation loss decreased (3.090924 --> 3.055941).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 2.334475 \tValidation Loss: 2.647303\n",
      "Validation loss decreased (3.055941 --> 2.647303).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 2.202648 \tValidation Loss: 2.792328\n",
      "Epoch: 16 \tTraining Loss: 2.086890 \tValidation Loss: 2.593165\n",
      "Validation loss decreased (2.647303 --> 2.593165).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 2.000966 \tValidation Loss: 2.529872\n",
      "Validation loss decreased (2.593165 --> 2.529872).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.877203 \tValidation Loss: 2.509229\n",
      "Validation loss decreased (2.529872 --> 2.509229).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.807992 \tValidation Loss: 2.419632\n",
      "Validation loss decreased (2.509229 --> 2.419632).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.677897 \tValidation Loss: 2.529030\n",
      "Epoch: 21 \tTraining Loss: 1.620662 \tValidation Loss: 2.530397\n",
      "Epoch: 22 \tTraining Loss: 1.517410 \tValidation Loss: 2.345759\n",
      "Validation loss decreased (2.419632 --> 2.345759).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.449576 \tValidation Loss: 2.261964\n",
      "Validation loss decreased (2.345759 --> 2.261964).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.346586 \tValidation Loss: 2.277415\n",
      "Epoch: 25 \tTraining Loss: 1.294510 \tValidation Loss: 2.272532\n",
      "Epoch: 26 \tTraining Loss: 1.239203 \tValidation Loss: 2.225885\n",
      "Validation loss decreased (2.261964 --> 2.225885).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.149876 \tValidation Loss: 2.263484\n",
      "Epoch: 28 \tTraining Loss: 1.113268 \tValidation Loss: 2.294892\n",
      "Epoch: 29 \tTraining Loss: 1.058879 \tValidation Loss: 2.112551\n",
      "Validation loss decreased (2.225885 --> 2.112551).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.000255 \tValidation Loss: 2.175125\n",
      "Epoch: 31 \tTraining Loss: 0.946741 \tValidation Loss: 2.145799\n",
      "Epoch: 32 \tTraining Loss: 0.896910 \tValidation Loss: 2.100714\n",
      "Validation loss decreased (2.112551 --> 2.100714).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.847855 \tValidation Loss: 2.194199\n",
      "Epoch: 34 \tTraining Loss: 0.791024 \tValidation Loss: 2.259154\n",
      "Epoch: 35 \tTraining Loss: 0.731074 \tValidation Loss: 2.205567\n",
      "Epoch: 36 \tTraining Loss: 0.710310 \tValidation Loss: 2.112502\n",
      "Epoch: 37 \tTraining Loss: 0.660599 \tValidation Loss: 2.126361\n",
      "Epoch: 38 \tTraining Loss: 0.628563 \tValidation Loss: 2.192683\n",
      "Epoch: 39 \tTraining Loss: 0.603316 \tValidation Loss: 2.190074\n",
      "Epoch: 40 \tTraining Loss: 0.564743 \tValidation Loss: 2.155403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following import is required for training to be robust to truncated images\n",
    "import numpy as np\n",
    "\n",
    "from PIL import ImageFile \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # calculate the train loss\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            # forward pass to predict the outputs\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output,target)\n",
    "            # update the average validation loss\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print(f\"Epoch: {epoch} \\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {valid_loss:.6f}\")\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f\"Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}).  Saving model ...\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "# train the model\n",
    "model_scratch = train(40, loaders_scratch, model_scratch, optimizer_scratch, \n",
    "                      criterion_scratch, use_cuda, 'model_scratch_test.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_scratch.load_state_dict(torch.load('model_scratch_test.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.038834\n",
      "\n",
      "\n",
      "Test Accuracy: 48% (402/836)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
